import shutil

import yaml

from kedro_databricks.core.constants import DEFAULT_CONFIG_KEY, DEFAULT_TARGET
from kedro_databricks.core.utils import get_arg_value
from kedro_databricks.plugin import commands


def _validate_bundle(
    metadata,
    required_files,
    task_validator,
):
    """Validate the resources generated by the bundle command."""
    resources_dir = metadata.project_path / "resources"
    assert resources_dir.exists(), "Resources directory not created"
    assert resources_dir.is_dir(), "Resources directory is not a directory"
    for file in resources_dir.iterdir():
        assert file.is_file(), f"{file} is not a file"
        assert file.suffix in (".yml", ".yaml"), f"{file} is not a YAML file"
        assert file.name in required_files, f"{file} is not a required file"
        with open(file) as f:
            resource = yaml.safe_load(f)
        assert "resources" in resource, f"'resources' key not found in {file}"
        if file.name.startswith("jobs."):
            assert len(resource["resources"]["jobs"]) == 1, (
                f"Expected 1 job in {file}, found {len(resource['resources']['jobs'])}"
            )
            job_name = file.name.split(".")[1]
            job = resource["resources"]["jobs"].get(job_name)
            assert job is not None, f"Job {job_name} not found in {file}"
            tasks = job.get("tasks")
            assert tasks is not None, f"Tasks not found in job {file}"
            task_validator(tasks)


def test_databricks_bundle_fail(kedro_project_with_init):
    metadata, cli_runner = kedro_project_with_init
    bundle_fail = ["databricks", "bundle", "--default-key", "_deault"]
    result = cli_runner.invoke(commands, bundle_fail, obj=metadata)
    assert result.exit_code == 1, (result.exit_code, result.stdout, result.exception)


def test_databricks_bundle_with_overrides(kedro_project_with_init):
    metadata, cli_runner = kedro_project_with_init
    command = ["databricks", "bundle", "--env", DEFAULT_TARGET]
    result = cli_runner.invoke(commands, command, obj=metadata)
    assert result.exit_code == 0, (result.exit_code, result.stdout, result.exception)

    def task_validator(tasks):
        assert len(tasks) == 8
        for i, task in enumerate(tasks):
            assert task.get("task_key") in (f"node{i}", f"ns_{i}_node_{i}_1")
            assert task.get("environment_key") == DEFAULT_CONFIG_KEY
            params = task.get("python_wheel_task").get("parameters")
            assert params is not None
            env = get_arg_value(params, "--env")
            assert env is not None
            assert env == "${var.environment}"

    _validate_bundle(
        metadata=metadata,
        required_files=[
            f"jobs.{metadata.package_name}.yml",
            f"jobs.{metadata.package_name}_ds.yml",
            f"jobs.{metadata.package_name}_namespaced_pipeline.yml",
            f"volumes.{metadata.package_name}_volume.yml",
        ],
        task_validator=task_validator,
    )


def test_databricks_bundle_with_conf(kedro_project_with_init):
    """Test the `bundle` command"""
    metadata, cli_runner = kedro_project_with_init
    command = [
        "databricks",
        "bundle",
        "--env",
        DEFAULT_TARGET,
        "--conf-source",
        "conf/sub_pipeline",
    ]
    result = cli_runner.invoke(commands, command, obj=metadata)
    assert result.exit_code == 0, (result.exit_code, result.stdout, result.exception)

    def task_validator(tasks):
        assert len(tasks) == 8
        for i, task in enumerate(tasks):
            assert task.get("task_key") in (f"node{i}", f"ns_{i}_node_{i}_1")
            assert task.get("environment_key") == DEFAULT_CONFIG_KEY
            params = task.get("python_wheel_task").get("parameters")
            assert params is not None
            env = get_arg_value(params, "--env")
            assert env is not None
            assert env == "${var.environment}"

    _validate_bundle(
        metadata=metadata,
        required_files=[
            f"jobs.{metadata.package_name}.yml",
            f"jobs.{metadata.package_name}_ds.yml",
            f"jobs.{metadata.package_name}_namespaced_pipeline.yml",
            f"volumes.{metadata.package_name}_volume.yml",
        ],
        task_validator=task_validator,
    )


def test_databricks_bundle_without_overrides(kedro_project_with_init):
    """Test the `bundle` command"""
    metadata, cli_runner = kedro_project_with_init
    command = ["databricks", "bundle", "--env", DEFAULT_TARGET]
    result = cli_runner.invoke(commands, command, obj=metadata)
    assert result.exit_code == 0, (result.exit_code, result.stdout, result.exception)

    def task_validator(tasks):
        assert len(tasks) == 8
        for i, task in enumerate(tasks):
            assert task.get("task_key") in (f"node{i}", f"ns_{i}_node_{i}_1")
            assert task.get("environment_key") == DEFAULT_CONFIG_KEY
            params = task.get("python_wheel_task").get("parameters")
            assert params is not None
            env = get_arg_value(params, "--env")
            assert env is not None
            assert env == "${var.environment}"

    _validate_bundle(
        metadata=metadata,
        required_files=[
            f"jobs.{metadata.package_name}.yml",
            f"jobs.{metadata.package_name}_ds.yml",
            f"jobs.{metadata.package_name}_namespaced_pipeline.yml",
            f"volumes.{metadata.package_name}_volume.yml",
        ],
        task_validator=task_validator,
    )


def test_databricks_bundle_with_params(kedro_project_with_init):
    """Test the `bundle` command"""
    metadata, cli_runner = kedro_project_with_init
    command = [
        "databricks",
        "bundle",
        "--env",
        DEFAULT_TARGET,
        "--params",
        "run_date={{job.parameters.run_date}},run_id={{job.parameters.run_id}}",
        "--overwrite",
    ]
    result = cli_runner.invoke(commands, command, obj=metadata)
    assert result.exit_code == 0, (result.exit_code, result.stdout, result.exception)

    def task_validator(tasks):
        assert len(tasks) == 8
        for i, task in enumerate(tasks):
            assert task.get("task_key") in (f"node{i}", f"ns_{i}_node_{i}_1")
            assert task.get("environment_key") == DEFAULT_CONFIG_KEY
            params = task.get("python_wheel_task").get("parameters")
            assert params is not None
            value = get_arg_value(params, "--params")
            assert value is not None
            assert (
                value
                == "run_date={{job.parameters.run_date}},run_id={{job.parameters.run_id}}"
            )

    _validate_bundle(
        metadata=metadata,
        required_files=[
            f"jobs.{metadata.package_name}.yml",
            f"jobs.{metadata.package_name}_ds.yml",
            f"jobs.{metadata.package_name}_namespaced_pipeline.yml",
            f"volumes.{metadata.package_name}_volume.yml",
        ],
        task_validator=task_validator,
    )


def test_databricks_bundle_with_pipeline(kedro_project_with_init):
    """Test the `bundle` command"""
    metadata, cli_runner = kedro_project_with_init
    if (metadata.project_path / "resources").exists():
        shutil.rmtree(metadata.project_path / "resources")
    command = [
        "databricks",
        "bundle",
        "--env",
        DEFAULT_TARGET,
        "--pipeline",
        "ds",
        "--overwrite",
    ]
    result = cli_runner.invoke(commands, command, obj=metadata)
    assert result.exit_code == 0, (result.exit_code, result.stdout, result.exception)

    def task_validator(tasks):
        assert len(tasks) == 8
        for i, task in enumerate(tasks):
            assert task.get("task_key") in (f"node{i}", f"ns_{i}_node_{i}_1")
            assert task.get("environment_key") == DEFAULT_CONFIG_KEY
            params = task.get("python_wheel_task").get("parameters")
            assert params is not None
            env = get_arg_value(params, "--env")
            assert env is not None
            assert env == "${var.environment}"
            pipeline = get_arg_value(params, "--pipeline")
            assert pipeline is None

    _validate_bundle(
        metadata=metadata,
        required_files=[
            f"jobs.{metadata.package_name}_ds.yml",
            f"volumes.{metadata.package_name}_volume.yml",
        ],
        task_validator=task_validator,
    )


def test_databricks_bundle_with_no_nodes(kedro_project_with_init):
    """Test the `bundle` command"""
    metadata, cli_runner = kedro_project_with_init
    command = [
        "databricks",
        "bundle",
        "--env",
        DEFAULT_TARGET,
        "--pipeline",
        "non-existing-pipeline",
        "--overwrite",
    ]
    result = cli_runner.invoke(commands, command, obj=metadata)
    assert result.exit_code == 1, (result.exit_code, result.stdout, result.exception)
    assert isinstance(result.exception, KeyError)
    exception = bytes(str(result.exception), "utf-8").decode("unicode_escape")
    expected = "Pipeline 'non-existing-pipeline' not found."
    assert expected in result.exception.args[0], (
        f"Expected exception message: {expected} not found in {exception}"
    )
